### 아파치 카프카 개요 및 설명

#### Data전송하는 Source Application VS Data 전송받는 Target Application

#### Before
1. 초기에는 간단한 단방향 통신
2. 어플리케이션이 많아지면서 데이터 전송 구조가 복잡해짐 
3. 데이터 전송 라인이 많아짐으로써 배포와 오류 대처에 문제 발생

#### Kafka 역할
아파치 카프카는 Source Application과 Target Application의 커플링을 약하게 한다.

Source Application은 Kafka로 데이터를 전송하고 Target Application은 Kafka에서 데이터를 가져온다.

Source Application에서 보내는 데이터 포맷은 거의 제한이 없다.(Json, tsv, avro ...)

Kafka는 데이터를 저장하는 Topic이란 개념이 있는데 일종의 큐라고 볼 수 있다.

Kafka Topic에 데이터를 저장하는 Producer와 데이터를 가져가는 Consumer가 있는데 Source Application을 Producer, Target Application을 Consumer로 볼 수 있다.

고가용성으로 서버에 이슈가 생기거나 전원이 내려간 상황에서도 데이터를 손실 없이 복구 할 수 있다.

낮은 지연과 높은 처리량으로 빅데이터 처리에 유용하다.


### 토픽이란?

카프카에는 다양한 형태의 데이터가 들어갈 수 있는데 이 때 데이터가 저장되는 곳을 토픽이라고한다

카프카에서의 토픽은 일종의 폴더라고 생각할 수 있고, 토픽을 여러개 생성 할 수 있다. 

프로듀서는 토픽에 데이터를 저장하고 컨슈머는 토픽에 저장된 데이터를 가져간다.

토픽은 이름을 가질 수 있고 목적에 따라 click_log, send_sms 등 명확한 용도를 기재할 수 있다.

하나의 토픽은 여러개의 파티션을 가질 수 있으며 이때 파티션은 Index 0번부터 시작된다.

하나의 파티션은 큐와 같이 내부에 데이터가 끝에서부터 차곡차곡(Index 0 시작)쌓이게 된다.

컨슈머는 가장 오래된 순서부터 데이터를 가져가게 된다(Index 0번부터 가져감)

`이 때 컨슈머가 데이터(record)를 가져가더라도 데이터는 삭제되지 않는다.`

카프카에서는 읽은 데이터를 토픽에서 삭제하지 않기 때문에

1. 컨슈머 그룹이 다르고

2. auto.offset.reset = earliest

로 설정된 경, 새로운 컨슈머가 다시 0번부터 동일 데이터를 처리할 수 있다.

예를들어 동일한 데이터에 대해 1번 컨슈머 그룹에 속한 컨슈머가 로그를 ES에 저장하고, 2번 컨슈머 그룹에 속한 컨슈머가 Hadoop에 저장할 수 있다.

### 파티션이 두개 이상인 경우

데이터를 보낼 때 키를 지정할 수 있다.

만약 키를 지정하지 않으면(null) `Round-Robin` 으로 파티션이 지정된다.

키가 있고 기본 파티셔너를 사용할 경우, 키의 해시(Hash)를 구하고 특정 파티션에 할당

### 파티션 추가 생성

파티션은 늘릴수는 있지만 줄일 수는 없기 때문에 파티션을 늘리는 것은 신중해야한다

#### 파티션을 늘리는 이유

파티션을 늘리면 컨슈머 개수를 늘려서 데이터 분산 처리 가능

#### 파티션의 데이터(record) 삭제 시점

삭제 되는 타이밍은 옵션에 따라 다르다.

record가 저장되는 최대 시간과 크기를 지정할 수 있다

log.retention.ms : 최대 record 보존 시간
log.retention.byte : 최대 record 보존 크기(byte)